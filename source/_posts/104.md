---
title: 【Linux存储系列教程】ceph的架构和原理
date: 2023-05-06 13:37:35.298
updated: 2023-05-06 13:37:35.298
categories: 
- 服务器搭建
- 笔记
- linux教程
- linux基础服务
- 集群
- 存储
- ceph
tags: 
- linux
- 集群
- 分布式存储集群
- ceph
---

# 【Linux存储系列教程】ceph的架构和原理

## 一、ceph的介绍

- Ceph是一个==统一==的分布式存储系统，设计初衷是提供较好的性能、可靠性和可扩展性。
- 目前已得到众多云计算厂商的支持并被广泛应用。RedHat及OpenStack都可与Ceph整合以支持虚拟机镜像的后端存储。

## 二、ceph的特性

- 高性能
	- 采用`CRUSH`算法实现数据均衡分布
	- 支持上千存储节点，支持TB到PB级的数据
- 高可用性
- 高可扩展性
- 特性丰富
	- 支持三种存储接口：==块存储==、==文件系统存储==、==对象存储==
	- 支持==多种语言==驱动

![ceph01](https://www.wsjj.top/upload/2023/05/ceph01.png)

## 三、ceph的核心概念

![ceph02](https://www.wsjj.top/upload/2023/05/ceph02.png)

### 1.Monitor

>**一个`ceph`集群需要多个`monitor`集群，它们通过`paxos`同步数据**
**用于保存`OSD`元数据**

### 2.OSD

>**`Object Storage Device`对象存储设备**
**负责响应客户端请求、返回具体数据的进程 **
**一个`ceph`集群一般都存在多个`OSD`**

### 3.MDS

>**`Ceph Metadata Server`是`cephFS`依赖的元数据服务**

### 4.Object

>**`ceph`最底层的存储单元**
**每个`object`包含元数据和原始数据**

### 5.PG

>**`Placement Groups`一个`PG`包含多个`OSD`**
**为了更好地分配、定位数据**

### 6.RADOS

>**`Reliable Automatic Distributed Object Store`实现数据分配、`Failover`等集群操作**

### 7.Libradio

>**访问`RADIO`的库**
**提供`PHP`、`Ruby`、`Java`、`Python`、`C`和`C++`支持**

### 8.CRUSH

>**`ceph`使用的数据分布式算法 **

### 9.RBD

>**`RADOS Block device`**
**`ceph`对外提供的块设备服务**

### 10.RGW

>**`RADOS gateway`**
**`ceph`对外提供对象存储服务**

### 11.cephFS

>**`ceph file system`**
**`ceph`对外提供的文件系统服务**

## 四、ceph IO流程

![ceph03](https://www.wsjj.top/upload/2023/05/ceph03.png)

### 1.正常IO流程

![ceph04](https://www.wsjj.top/upload/2023/05/ceph04.png)

1. `client`创建`cluster handler`。
1. `client`读取配置文件。
1. `client`连接上`monitor`，获取集群`map`信息。
1. `client`读写`io`根据`crshmap`算法请求对应的主`osd`数据节点。
1. 主`osd`数据节点同时写入另外两个副本节点数据。
1. 等待主节点以及另外两个副本节点写完数据状态。
1. 主节点及副本节点写入状态都成功后，返回给`client`，`io`写入完成。

### 2.新主IO流程

>如果新加入的OSD1取代了原有的 OSD4成为 Primary OSD, 由于 OSD1 上未创建 PG , 不存在数据，那么 PG 上的 I/O 无法进行，怎样工作的呢？

![ceph05](https://www.wsjj.top/upload/2023/05/ceph05.png)

1. `client`连接`monitor`获取集群`map`信息。
1. 同时新主`osd1`由于没有`pg`数据会主动上报`monitor`告知让`osd2`临时接替为主。
1. 临时主`osd2`会把数据全量同步给新主`osd1`。
1. `client IO`读写直接连接临时主`osd2`进行读写。
1. `osd2`收到读写`io`，同时写入另外两副本节点。
1. 等待`osd2`以及另外两副本写入成功。
1. `osd2`三份数据都写入成功返回给`client`, 此时`client io`读写完毕。
1. 如果`osd1`数据同步完毕，临时主`osd2`会交出主角色。
1. `osd1`成为主节点，`osd2`变成副本。

### 3.ceph IO算法流程

![ceph06](https://www.wsjj.top/upload/2023/05/ceph06.png)

#### `File`用户需要读写的文件`File->Object`映射：

1. `ino`File的元数据，File的唯一id
1. `ono`File切分产生的某个object的序号，默认以4M切分一个块大小
1. `oid`object id: ino + ono

#### `Object`是`RADOS`需要的对象。`Ceph`指定一个静态`hash`函数计算`oid`的值，将`oid`映射成一个近似均匀分布的伪随机值，然后和`mask`按位相与，得到`pgid`  `Object->PG`映射：

1. hash(oid) & mask-> pgid 
1. mask = PG总数m(m为2的整数幂)-1 

#### PG(Placement Group),用途是对`object`的存储进行组织和位置映射, (类似于redis cluster里面的slot的概念) 一个`PG`里面会有很多`object`。采用`CRUSH`算法，将`pgid`代入其中，然后得到一组`OSD`。`PG->OSD`映射：

1. CRUSH(pgid)->(osd1,osd2,osd3) 。

## 五、ceph心跳机制

### 1.OSD节点监听端口

- `OSD`节点会监听`public`、`cluster`、`front`和`back`四个端口
	- `public`端口，监听来自`Monitor`和`Client`的连接 
	- `cluster`端口，监听来自`OSD peer`的连接 
	- `front`端口，供客户端连接集群使用的网卡，临时给集群内部间进行心跳
	- `back`端口，供集群内部使用的端口，用于集群内部间进行心跳
	- `hbclient`端口，发送`ping`心跳的`message`

### 2.OSD之间的心跳

![ceph07](https://www.wsjj.top/upload/2023/05/ceph07.png)

- 同一个`PG`内`OSD`互相心跳，他们互相发送`PING/PONG`信息。
- 每隔`6s`检测一次(实际会在这个基础上加一个随机时间来避免峰值)。
- `20s`没有检测到心跳回复，加入`failure`队列。

### 3.OSD与Monitor间的心跳

![ceph08](https://www.wsjj.top/upload/2023/05/ceph08.png)

- `OSD`有事件发生时（比如故障、PG变更）。
- 自身启动`5`秒内。
- `OSD`周期性的上报给`Monitor`
	- `OSD`检查`failure_queue`中的伙伴`OSD`失败信息。
	- 向`Monitor`发送失效报告，并将失败信息加入`failure_pending`队列，然后将其从`failure_queue`移除。
	- 收到来自`failure_queue`或者`failure_pending`中的`OSD`的心跳时，将其从两个队列中移除，并告知`Monitor`取消之前的失效报告。
	- 当发生与`Monitor`网络重连时，会将`failure_pending`中的错误报告加回到`failure_queue`中，并再次发送给`Monitor`。
- `Monitor`统计下线`OSD`
	- `Monitor`集来自`OSD`的伙伴失效报告。
	- 当错误报告指向的`OSD`失效超过一定阈值，且有足够多的`OSD`报告其失效时，将该`OSD`下线。
